"Hello, this is the time to work"
"Life is this, I like this"
"I dont have dreams, I have goals and now I am aiming to the next one"
"Neural net architectures form a flexible framework that can be used to construct 
many different types of classifiers. These include Gaussian, k-nearest neighbor, and 
multi-layer perceptton classifiers as well as classifiers such as the feature map classifier 
which use unsupervised training. Here we first demonstrated that two-layer percepttons 
(one hidden layer) can form non-convex and disjoint decision regions. Back propagation 
training, however, can be extremely slow when forming complex decision regions with 
multi-layer perceptrons. Alternative classifiers were thus developed and tested. All 
provided faster training and many provided improved performance. Two were similar to 
traditional classifiers. One (hypercube classifier) can be used to implement a histogram 
classifier, and another (feature map classifier) can be used to implement a k-nearest 
neighbor classifier. The feature map classifier provided best overall performance. It 
used combined supervised/unsupervised training and attained the same error rate as a 
k-nearest neighbor classifier, but with fewer supervised training tokens. Furthermore, 
it required fewer nodes then a k-nearest neighbor classifier."
"The result imposes a lower bound on the connectivity of the network (num- ber of synapses per neuron). This lower bound can only be a consequence of the learning aspect, since switching theory provides purposely designed circuits of low connectivity (e.g., using only two-input NAND gates) capable of imple- menting any Boolean function [1,2]. It also follows that the learning mechanism must be restricted for this lower bound to hold; a powerful mechanism can be @ American Institute of Physics 1988 2 designed that will find one of the low-connectivity circuits (perhaps by exhaus- tive search), and hence the lower bound on connectivity cannot hold in general. Indeed, we restrict the learning mechanism to be local; when a training sample is loaded into the network, each neuron has access only to those bits carried by itself and the neurons it is directly connected to. This is a strong assumption that excludes sophisticated learning mechanisms used in neural-network models, but may be more plausible from a biological point of view. The lower bound on the connectivity of the network is given in terms of the entropy of the environment that provides the training samples. Entropy is a quantitative measure of the disorder or randomness in an environment or, equiv- alently, the amount of information needed to specify the environment. There are many different ways to define entropy, and many technical variations of this concept [3]. In the next section, we shall introduce the formal definitions and results, but we start here with an informal exposition of the ideas involved. The environment in our model produces patterns represented by N bits x = x... xN (pixels in the picture of a visual scene if you will). Only h different patterns can be generated by a given environment, where h < 2 v (the entropy is essentially log 2 h). No knowledge is assumed about which patterns the en- vironment is likely to generate, only that there are h of them. In the learning process, a huge number of sample patterns are generated at random from the environment and input to the network, one bit per neuron. The network uses this information to set its internal parameters and gradually tune itself to this particular environment. Because of the network architecture, each neuron knows only its own bit and (at best) the bits of the neurons it is directly connected to by a synapse. Hence, the learning rules are local: a neuron does not have the benefit of the entire global pattern that is being learned. "
"A way to extend the backpropagation method to feedback networks has been proposed. A condition on the weight matrix is obtained, to insure having only one fixed point, so as to prevent having more than one possible output for a fixed input. A general structure for networks is presented, in which the network consists of a number of feedback groups connected to each other in a feedforward manner. A stochastic descent rule is used to update the weights. The method is applied to a pattern recognition example. With a single-layer feedback network i obtained good results. On the other hand, the feedforward backpropagation method achieved good resuls only for the case of more than one layer, hence also with a larger number of neurons than the feedback case."
"The notion of convolution or correlation used in the models presented is popular in engineering disciplines and has been applied extensively to designing filters, control systems, etc. Such operations also occur in biological systems and have been applied to modeling neural net- works.15,16 Thus the concept of dynamic formal neuron may be helpful for the improvement of artificial neural network models as well as the understanding of biological systems. A portion of the system described by Tank and Hopfield 17 is similar to the matched filter bank model simu- lated in this paper. The matched filter bank model (Model 1) performs well when all phonemes (as above) are of the same duration. Otherwise, it would perform poorly unless the lengths were forced to a maximum length by padding the input and transfer functions with -l's during calculation. The pseudo-inverse filter model, on the other hand, should not suffer from this problem. However, this aspect of the model (Model 2) has not yet been explicitly simulated. Given a spatio-temporal pattern of size L x K, i.e., L spatial elements and K temporal ele- ments, the number of calculations required to process the first stage of filtering by both models is the same as that by a static formal neuron network in which each neuron is connected to the L x K input elements. In both cases, L x K multiplications and additions are necessary to calculate one output value. In the case of bipolar patterns, the mutiplication used for calculation of activa- tion can be replaced by sign-bit check and addition. A future investigation is to use recursive filters or analog filters as transfer functions for faster and more efficient calculation. There are various schemes to obtain optimal recursive or analog filters. is, 19 Besides the lateral inhibition scheme used in the models, there are a number of alternative procedures to realize a ""winner- take-all"" network in analog or digital fashion. 15,2ð,21 "
"We consider homogeneous algebraic threshold networks whose weights w.r are i.i.d., fir(0, 1) random vaxiables. This is a natural generalisation to higher-order of Ising spin glasses with Gaussian interactions. We will show an asymptotic estimate for the number of fixed points of the structure. Asymptotic results for the usual case d = i of linear threshold networks with Gaussian interactions have been reported in the literature"
"criterion the net learns very little from this example, whereas, us- ing eqn(3), Olog(p)/Ofj = 1/(1 -), so the net continues to learn and can in fact converge to predict probabilities near 1. Indeed because backpropagation using the standard 'Error' measure can not converge to generate outputs of 1 or 0, it has been custom- ary in the literature 4 to round the target values so that a target of i would be presented in the learning algorithm as some ad hoc number such as .8, whereas a target of 0 would be presented as .2. In the context of our general discussion it is natural to ask whether using a feedforward network and varying the weights is in fact the most effective alternative. Anderson and Abrahams 3 have discussed this issue from a Bayesian viewpoint. From this point of view, fitting output to input using normal distributions and varying the means and covariance matrix may seem to be more logical. Feedforward networks do however have several advantages for complex problems. Experience with neural networks has shown the importance of including hidden units wherein the network can form an internal representation of the world. If one simply uses normal distributions, any hidden variables included will simply integrate out in calculating an output. It will thus be necessary to include at least third order correlations to implement useful hidden variables. Unfortunately, the number of possible third order correlations is very large, so that there may be practical obstacles to such an approach. Indeed it is well known folklore in curve fitting and pattern classification that the number of parameters must be small compared to the size of the data set if any generalization to future cases is expected"
"Constraints on the Orientation Map Our original definition of the orientation map permits each cell to have an orien- tation preference whose angle is completely independent of its neighbors. But this is much too general. Looking at the results of tangential electrode penetrations, there are two striking constraints in the data. The first of these is reflected in the smoothness of the graphs. Orientation changes in a regular manner as the electrode moves horizon- tally through the upper layers: neighboring cells have similar orientation preferences. Discontinuities do occur but are rare. The other constraint is the fact that the orienta- tion is always changing with distance, although the rate of change may vary. Sequences of constant orientation are very rare and when they do occur they never carry on for any appreciable distance. This is one of the major reasons why the paral- lel stripe model is untenable."
"the weight changes are accumulated over all 4 input/output cases, and only networks with continuous units are considered. Also in this case, the ð1 units lead to an improved learning behavior (the optimal q-values are about 2.5 and 5.0, respectively). They not only lead to significantly smaller learning times, but ð1 networks also appear to be less sensitive with respect to a variation of q than the corresponding 0,1 versions. The better performance of the ð1 models with continuous units can partly be attributed to the steeper slope of the chosen activa- tion function, Eq. (3). A comparison with activation functions that have the same slope, however, shows that the networks with ð1 units still perform significantly better than those with 0,1 units. If the weights are updated after every learning example, e.g., the reduc- tion in learning time remains as large as a factor of 5. In the case of backpropagation learning, the main reason for the better perfor- mance of ð1 units thus seems to be related to the fact that the algorithm does not modify weights which emerge from a unit with value zero. Similar observations have been made by Stornetta and Huberman, s who further find that the discrepancies become even more pronounced if the network size is increased"
"In this paper we have considered the all-or-nothing contribution of the var- ious channels, i.e. the entire population of a given channel type is either activated normally or all the channels are disabled/blocked. This descrip- tion may be oversimplified in two ways. First, it is possible that a blocking mechanism for a given channel may have a graded effect. For example, it is possible that cholinergic input is not homogeneous over the soma membrane, or that at a given time only a portion of these afferents are activated. In either case it is possible that only a portion of the cholinergic receptors are bound, thus inhibiting a portion of channels. Second, the result of channel inhibition by neuromodulatory projections must consider both single cell 3The slowing down of the spike trains in Figure 2 and Figure 3 is mainly due to the buildup of [Ca2+]i,, which progressively activates more IAHr. 93 response and population response, the size of the population depending on the neuro-architecture of a cortical region and the afferents. For example, activation of a cholinergic tract which terminates in a localized hippocampal region may effect thousands of HPCs. Assuming that the IM of individual HPCs in the region may be either turned on or off completely with some probability, the behavior of the population will be that of a graded response of IM inhibition. This graded response will in turn depend on the strength of the cholinergic tract activity. The key point is that the information processing properties of isolated neurons may be reflected in the behavior of a population, and vica-versa. While it is likely that removal of a single pyramidal cell from the hippocam- pus will have zero functional effect, no neuron is an island. Understand- ing the central nervous system begins with the spectrum of behavior in its functional units, which may range from single channels, to specific areas of a dendritic tree, to the single cell, to cortical or nuclear subfields, on up through the main subsystems of CNS. "
"The techniques we have just described were tested in the following way. For the first experiment we identified two spike classes in a recording from the rat cerebellum. A signal is created, composed of a number of spikes from the two classes at random instants, plus noise. To make the situation as realistic as possible, the added noise is taken from idle periods (i.e. non-spiking) of a real recording. The reason for using such an artificially generated signal is to be able to know the class identities of the spikes, in order to test our approach quantitatively. We implement the detection and classification techniques on the obtained signal, with various values of noise amplitude. In our case the ratio of the peak to peak values of the telnplates turns out to be 1.375. Also, the spike rate of one of the clases is twice that of the other class. Fig.3a shows the results with applying the first scheme (i.e. using Eq. 3). The overall percentage correct classification for all spikes (solid curve) and the percentage correct classification for overlapping spikes (dashed curve) are plotted versus the standard deviation of the noise cr normalized with respect to the peak h of the large template. Notice that the overall classification accuracy is near 100% for cr/h less than 0.15, which is actually the range of noise amplitudes we mostly encountered in our work with real recordings. Observe also the good results for classifying overlapping events. We have applied also the second scheme (i.e. using Eq. 4) and obtained similar results. We wish to mention that the thresholds for detection and for the reject option are set up so as to obtain no more than 3% falsely detected spikes. A similar experiment is performed with three waveforms (three classes), where two of the waveforms are the same as those used in the first experiment. The third is the average of the first two. All the three neurons have the same spike rate (i.e. ,l : -2 = ,3). Hence both classification schemes are equivalent in this case. Fig. 3b shows the overall as well as the sub-category of overlap classification results. One observes that the results are worse than those for the two-class case. This is because the spacings between the templates are in general smaller. Notice also that the accuracy in resolving overlapping events is now tangibly less than the overall accuracy. However, one can say that the results are acceptable in the range of cr/h less than 0.1. The following experiment is also performed using the same data. We would like to investigate the importance of the information given by the (overall) firing rate on the problem of classifying overlapping events. In our method the summation in the likelihood functions for single spikes is multiplied by o/n, while that for overlapping spikes is multiplied by (c/n) 2. Usually ot/n is considerably less than one. Hence we have a factor which gives less weight for overlapping events. Now, consider the case of ignoring completely the information given by the firing rate and relying solely on shape information. We assume that overlapping spikes from any two given classes represent ""new"" class of waveforms and that each of these overlap classes has the same rate as that of a single-spike class. In that case we can obtain expressions for the likelihood functions as consisting just the summations"
"To compare the performance of each of the three approaches, we devised a common set of test data using the following procedures. First, we used the prin- cipal component method of Abeles and Goldstein 3 to generate two templates from a digitized analog record of neural activity recorded in the cerebellum of the rat. The two actual spike waveform templates we decided to use had a peak-to-peak ratio of 1.375. From a second set of analog recordings made from a site in the cerebellum in which no action potential events were evident, we determined the spectral characteristics of the recording noise. These two components derived from real neural recordings were then digitally combined, the objective being to construct realistic records, while also knowing absolutely what the correct solution to the template matching problem was for each oc- curring spike. As shown in Fig. 2c and 2d, data sets corresponding to different noise to signal ratios were constructed. We also carried out simulations with the amplitudes of the templates themselves varied in the synthesized records to simulate waveform changes due to brain movements often seen in real record- ings. In addition to two waveform test sets, we also constructed three waveform sets by generating a third template that was the average of the first two tem- plates. To further quantify the comparisons of the three diffferent approaches described above we considered non-overlapping and overlapping spikes sepa- rately. To quantify the performance of the three different approaches, two standards for classification were devised. In the first and hardest case, to be judged a correct classification, the precise order and timing of two waveforms had to be reconstructed. In the second and looser scheme, classification was judged correct if the order of two waveforms was correct but timing was al- lowed to vary by 100 secs(i.e. +2 time bins) which for most neurobiological applications is probably sufficient resolution. Figs. 3-4 compare the perfor- mance results for the three approaches to waveform classification implemented as digital simulations. "